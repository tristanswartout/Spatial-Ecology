---
title: "R Notebook"
output: html_notebook
---

# Re-running code from lab as a starting point

```{r, warning=F}
require(terra)
require(tidyterra)
require(sf)
require(adehabitatHR)
require(adehabitatLT)
require(adehabitatHS)
require(tidyverse)
require(survival)


#Import landcover tif
land = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week10/panther_landcover.tif')

#Reclassify the landcover tif
classification = read.table('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week10/landcover%20reclass.txt', header=T) 
land = classify(land, classification[,c(1,3)])
land = categories(land, value=unique(classification[,c(3,4)]))


#Import panther locations
panthers = st_read('/vsicurl/https://github.com/ValenteJJ/SpatialEcology/raw/main/Week10/panthers.shp') %>% 
  mutate(CatID = as.factor(CatID))

#Calculate wet forest focal statistic (5 km radius)
wetForest = land
values(wetForest) = 0
wetForest[land %in% c(10,12)] = 1
probMatrix = focalMat(wetForest, 5000, type='circle', fillNA=FALSE)
wetFocal = focal(wetForest, probMatrix, fun='sum', na.rm=T)


#Calculate dry forest focal statistic (5 km radius)
dryForest = land
values(dryForest) = 0
dryForest[land %in% c(11, 13)] = 1
probMatrix = focalMat(dryForest, 5000, type='circle', fillNA=FALSE)
dryFocal = focal(dryForest, probMatrix, fun='sum', na.rm=T)

#Stack together 
layers = c(land, wetFocal, dryFocal)
names(layers) = c('landcover', 'wetForest', 'dryForest')

#Recreate our used points object
use = terra::extract(layers, panthers) %>% 
  data.frame() %>% 
  mutate(CatID = as.factor(panthers$CatID)) %>% 
  group_by(CatID, landcover) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  arrange(landcover) %>% 
  pivot_wider(names_from = landcover, values_from = n, values_fill=0) %>% 
  data.frame()
row.names(use) = use$CatID
use$CatID = NULL

#Recreate our available points object for a type II design
set.seed(8)
randII = spatSample(land, size=1000, as.points=T)
randIILand = data.frame(randII)

availII = randIILand %>% 
  group_by(Description2) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  rename(landcover = Description2) %>% 
  filter(!(is.na(landcover) | landcover=='Exotics')) %>% 
  pivot_wider(names_from = landcover, values_from = n)
```


# Challenge 1 (5 points)

In the lab, we estimated Manly's statistic (wi) values for a type II study design. We also fit a logistic regression for a type II study design. For this challenge, you're going to explore the relationship between wi values and beta values from a logistic regression model. Below I have recreated the analysis for producing wi values. I've also reconstructed the dataset we used for fitting the logistic regression models (allCovs).

Fit a new logistic regression model where use is a function of landcover-1 (the -1 removes the intercept from the fitted model). Make sure this is the only covariate in the model. Exponentiate the coefficients from the fitted model and compare them to the wi values calculated for each landcover type. What do you notice? Explain the similarities and/or differences in how you would interpret the wi values and exponentiated coefficients.

Lets first run the model to calculate Wi

```{r}
#Recreating the wi analysis
selRatioII = widesII(u = use, 
                     a = as.vector(as.matrix(availII)),
                     avknown = F,
                     alpha = 0.05)

#Recreating the dataset for logistic regression
useCovs = terra::extract(layers, panthers) %>% 
  select(-ID) %>% 
  mutate(use=1)
backCovs = terra::extract(layers, randII) %>% 
  select(-ID) %>% 
  mutate(use=0)
allCovs = rbind(useCovs, backCovs) %>% 
  filter(!(is.na(landcover) | landcover=='Exotics')) %>% 
  mutate(landcover = as.factor(as.character(landcover)))

```

Now lets run the logistic regression model

```{r}
rsflandcovernoint = glm(use ~ landcover-1, family=binomial(link=logit), data=allCovs)
summary(rsflandcovernoint)
```

Lets exponentiate the betas of the logistic model

```{r}
exp(rsflandcovernoint$coefficients)
```

Now lets pull up our wi value

```{r}
selRatioII$wi

```
Answer 1:

The wi metric is showing what landcover types may be selected for with this data. Looking at the coefficients I cans see barren, dry prairie, scrub-shrub, cypress swamp, pineland, hardwood swamp and upland forest are all selected for because their values are greater than 1. For logistic regression its the exponentiated odd ratio so you would state times as likely. Look at significance I can see that coastal wetland, cropland, cypress swamp, hardwood swamp, open water, pasture grassland, pine land, upland forest and urban were all significant. However, we are looking at landcovers that were selected for so we should only examine landcover types with a significant postive odds ratio. In this case, cypress swamp, hardwood swamp, landcover pineland, and landcover upland forest were all significant. These mean a panther were greater than 1 times as likely to inhabit this cover type. In the case of cypress swamp panthers were 5 times as likely to select for these areas while the second highest was hardwood swamp at 3.44 times as likely. There are significant differences and similarities between these. To start both models indicate selection for cypress swamp, pineland, hardwood swamp, and upland forest. But manly selective measures also indicate selection for scrub-shrub, barren and dry prairie but these selections were low which may indicate no significance in the logistic model.

$\color{red}{\text{Pretty good. Basically, the exponentiated beta coefficients represent the odds ratios for the various cover types (i.e., the odds a point in that category is used divided by the odds is is not used). This is the same way that wi is calculated. The only difference here is that we're now including a random effect to account for non-independence among points selected by the same panther. +4}}$


# Challenge 2 (5 points)

In the lab, we used the distribution of step lengths and turning angles to help us devise potential steps each individual could have taken at each point in time. Instead of step lengths, build a histogram representing the distribution of step speeds in km/hr. When and why might you choose to sample from a distribution of step speeds to calculate potential step lengths rather than drawing from the distribution of step lengths itself?

Lets setup the as.ltraj object

```{r}
substrRight = function(x, n){
  substr(x, nchar(x) - n+1, nchar(x))
}

panthersSp = panthers %>% 
  mutate(Juldate = as.character(Juldate)) %>% 
  mutate(date = as.numeric(substrRight(Juldate, 3))) %>% 
  mutate(Date = as.Date(date, origin=as.Date("2006-01-01"))) %>% 
  mutate(Date = as.POSIXct(Date, "%Y-%m-%d", tz='')) %>% 
  as('Spatial')

pantherLtraj = as.ltraj(xy=coordinates(panthersSp), date=panthersSp$Date, id=panthersSp$CatID, typeII=T)

```

Now lets calculate speed for every panther 

```{r}
pantherLtraj[[1]]$hour=(pantherLtraj[[1]]$dt/3600)
pantherLtraj[[2]]$hour=(pantherLtraj[[2]]$dt/3600)
pantherLtraj[[3]]$hour=(pantherLtraj[[3]]$dt/3600)
pantherLtraj[[4]]$hour=(pantherLtraj[[4]]$dt/3600)
pantherLtraj[[5]]$hour=(pantherLtraj[[5]]$dt/3600)
pantherLtraj[[6]]$hour=(pantherLtraj[[6]]$dt/3600)

pantherLtraj[[1]]$km=(pantherLtraj[[1]]$dist/1000)
pantherLtraj[[2]]$km=(pantherLtraj[[2]]$dist/1000)
pantherLtraj[[3]]$km=(pantherLtraj[[3]]$dist/1000)
pantherLtraj[[4]]$km=(pantherLtraj[[4]]$dist/1000)
pantherLtraj[[5]]$km=(pantherLtraj[[5]]$dist/1000)
pantherLtraj[[6]]$km=(pantherLtraj[[6]]$dist/1000)


pantherLtraj[[1]]$speed=(pantherLtraj[[1]]$km/pantherLtraj[[1]]$hour)
pantherLtraj[[2]]$speed=(pantherLtraj[[2]]$km/pantherLtraj[[2]]$hour)
pantherLtraj[[3]]$speed=(pantherLtraj[[3]]$km/pantherLtraj[[3]]$hour)
pantherLtraj[[4]]$speed=(pantherLtraj[[4]]$km/pantherLtraj[[4]]$hour)
pantherLtraj[[5]]$speed=(pantherLtraj[[5]]$km/pantherLtraj[[5]]$hour)
pantherLtraj[[6]]$speed=(pantherLtraj[[6]]$km/pantherLtraj[[6]]$hour)

p1=pantherLtraj[[1]]
p2=pantherLtraj[[2]]
p3=pantherLtraj[[3]]
p4=pantherLtraj[[4]]
p5=pantherLtraj[[5]]
p6=pantherLtraj[[6]]
```

Combine the files

```{r}
allp=rbind(p1,p2,p3,p4,p5,p6)
```

Now plot the histogram 

```{r}
hist(allp$speed)
```

Answer 2:

It is important to consider speed in some situations because speed may help distinguish individual behavior. Speed may provide clues on the typical behavior of an individual it is just gradually moving about or is it going fast from point to point which may help affirm if step lengths would be predicted to be long or short.

$\color{red}{\text{Pretty good, although I wanted to year you comment on the fact that step speed distributions will be better for generating potential steps in situations where fixes are not constant in time. +4}}$

# Challenge 3 (5 points)

Path straightness is a metric we can use to evaluate how tortuous of a path a tracked animal took from one point to another. We calculate straightness as the straight line distance between two points divided by the length of the path actually taken. The resulting straightness statistic takes a value between 0 and 1 where 1 indicates a straight line path and 0 represents an infinitely tortuous path.

For each of the 6 panthers, calculate the straightness of the path between the first and last point recorded. To do that, first calculate the numerator for each panther as the straight-line distance between the start and end points. HINT: the coordinates for each point are in UTMs (meters from the Equator and meters from the Prime Meridian). With the x and y coordinates for two different points, you can calculate their straight-line distance using the Pythagorean theorem.

Next calculate the denominator for each panther. To do this, you can simply sum all of the step distances for that particular individual.

Now divide the numerator by the denominator. Which panther took the most tortuous path? Which took the least tortuous path?


First our panther traj objects 

```{r}
pantherLtraj[[1]]
pantherLtraj[[2]]
pantherLtraj[[3]]
pantherLtraj[[4]]
pantherLtraj[[5]]
pantherLtraj[[6]]
```

Now lets calculate the path straightness for each panther

```{r}
#Panther 1 x and y
711913.4-712916.8
250236.1-250378.9
p1d=sqrt(1003.4^2 + 142.8^2)
```

```{r}
#Panther 2 x and y
644599.2-655210.3
322008.9-339305
p2d=sqrt(10611.1^2 + 17296.1^2)
```

```{r}
#Panther 3 x and y
660779.5-661528.9
246952.5-254586.1
p3d=sqrt(749.4^2 + 7633.6^2)
```

```{r}
#Panther 4 x and y
666184.9-668109.3
292121.2-290853.5
p4d=sqrt(1924.4^2 + 1267.7^2)
```

```{r}
#Panther 5 x and y
668733.3-696436.9
262333.2-258735.1
p5d=sqrt(27703.6^2 + 3598.1^2)
```

```{r}
#Panther 6 x and y
625305.0-703739.1
231779.5-216222.9
p6d=sqrt(78434.1^2 + 15556.6^2)
```

Now let create the sum distance (denominator)

```{r}
sump1=sum(p1[,'dist'],na.rm = TRUE)
sump2=sum(p2[,'dist'],na.rm = TRUE)
sump3=sum(p3[,'dist'],na.rm = TRUE)
sump4=sum(p4[,'dist'],na.rm = TRUE)
sump5=sum(p5[,'dist'],na.rm = TRUE)
sump6=sum(p6[,'dist'],na.rm = TRUE)
```


Finally lets divide the values 

```{r}
pt1=p1d/sump1
pt2=p2d/sump2
pt3=p3d/sump3
pt4=p4d/sump4
pt5=p5d/sump5
pt6=p6d/sump6
```

Now we have all our values

```{r}
pt1
pt2
pt3
pt4
pt5
pt6
```
Answer 3

Panther 1 took the most tortuous path while panther 6 took the least tortuous path based on the calculated values.

$\color{red}{\text{Excellent. +5}}$

# Challenge 4 (5 points)

For each panther, calculate the frequency with which locations were recorded as points per day. Plot path straightness as a function of frequency (there should be 6 points on this figure, one per panther). What relationship do you notice between these two variables, and why might that pattern be occurring?

Lets first find the start and end dates and the difference in time

```{r}
p1t1=as.Date("2006-01-04")
p1t2=as.Date("2006-12-29")
difftime1=difftime(as.POSIXct(p1t1), as.POSIXct(p1t2, tz="UTC"), units="days")

p2t1=as.Date("2006-01-04")
p2t2=as.Date("2006-10-16")
difftime2=difftime(as.POSIXct(p2t1), as.POSIXct(p2t2, tz="UTC"), units="days")
difftime2+-(1/24)

p3t1=as.Date("2006-01-04")
p3t2=as.Date("2006-12-13")
difftime3=difftime(as.POSIXct(p3t1), as.POSIXct(p3t2, tz="UTC"), units="days")

p4t1=as.Date("2006-01-04")
p4t2=as.Date("2006-12-29")
difftime4=difftime(as.POSIXct(p4t1), as.POSIXct(p4t2, tz="UTC"), units="days")

p5t1=as.Date("2006-01-11")
p5t2=as.Date("2006-12-29")
difftime5=difftime(as.POSIXct(p5t1), as.POSIXct(p5t2, tz="UTC"), units="days")

p6t1=as.Date("2006-03-06")
p6t2=as.Date("2006-12-22")
difftime6=difftime(as.POSIXct(p6t1), as.POSIXct(p6t2, tz="UTC"), units="days")
```
The differences in days

```{r}
difftime1
difftime2
difftime3
difftime4
difftime5
```
Now our points taken versus days to calculate points per day

```{r}
freq1=127/359
freq2=85/285.04
freq3=118/343
freq4=131/359
freq5=129/352
freq6=123/291

freq1
freq2
freq3
freq4
freq5
freq6
```


Now we can plot it

```{r}
freq=c(0.354,0.298,0.344,0.365,0.365,0.423)
tort=c(0.0012,0.0197,0.0091,0.0023,0.0443,0.1562)
plot(tort~freq)
results=lm(tort~freq)
abline(results)
```
Answer 4:

At the moment there is no significant relationship that I can see; the line may be trending as a positive relationship but it has a p-value of 0.08. I believe this pattern is occurring because we are not getting points everyday instead every few days which provides much less info on movement. However, if the relationship was significant I would expect to see as point frequency increases path straigntess decreases because more points will better show that animals do not follow a straight path. 

$\color{red}{\text{Good enough. The outcome of this analysis turned out to be wonky for some reason. +5}}$
