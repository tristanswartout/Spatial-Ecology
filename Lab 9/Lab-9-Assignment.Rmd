---
title: "Swartout-Lab 9"
output: html_notebook
---

```{r, warning=F, message=F}

rm(list=ls())

require(sf)
require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(PresenceAbsence)
require(ecospat)
#Don't forget to load your other R packages!
```

# This first code chunk just recreates the maps we built in the lab.

```{r}

# Model building data
vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))



# Validation data
vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))



# Bringing in the covariates
elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')


# Resampling to make the covariate rasters match
mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

# Mesic forest within 1 km
probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')


# Building the raster stack
layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')




#Creating background points
set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Extracting covariates for our different points

presCovs = terra::extract(layers, vathPresXy)
absCovs = terra::extract(layers, vathAbsXy)
backCovs = terra::extract(layers, backXy)
valCovs = terra::extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
absCovs = data.frame(vathAbsXy, absCovs, pres=0)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

presCovs = presCovs[complete.cases(presCovs),]
absCovs = absCovs[complete.cases(absCovs),]
backCovs = backCovs[complete.cases(backCovs),]



# Combining presence and background data into one dataframe

backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')
colnames(absCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)
presAbsCovs = rbind(presCovs, absCovs)


valCovs = valCovs %>% mutate(VATH = vathVal$VATH)
```
Lets run the models

```{r}

# Fitting bioclim envelope model
tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)



# Fitting GLM
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')


# Fitting GAM
gamModel = gam(pres ~ s(canopy, k=6) + s(elev, k=6) + s(mesic1km, k=6) + s(precip, k=6), family='binomial', data=presBackCovs, method='ML')

gamMap = predict(layers, gamModel, type='response')


# Fitting boosted regression tree model

boostModel = gbm(pres ~ elev + canopy + mesic1km + precip, distribution='bernoulli', n.trees=100, interaction.depth=2, shrinkage=0.1, bag.fraction=0.5, data=presBackCovs)

boostMap = predict(layers, boostModel, type='response')
boostMap = mask(boostMap, layers$canopy)


# Fitting random forest model

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)


#Fitting maxent model

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')


maxentMap = predictMaxNet(maxentModel, layers, type='logistic')
```



# Challenge 1 (4 points)

In the lab, we fit 6 SDMs. We then calculated discrimination statistics for all 6 and a calibration plot for 1 of them. Create calibration plots for the remaining 5 models, and then make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best." Defend your answer.

Make a temporary dataframe 

```{r}
tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]
```

Now lets make the validation data

```{r}

valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1])

```
Now lets plot calibration plots


```{r}
calibration.plot(valData, which.model=1, N.bins=20, xlab='predicted', ylab='Observed', main='bioclim')


calibration.plot(valData, which.model=2, N.bins=20, xlab='predicted', ylab='Observed', main='glm')

calibration.plot(valData, which.model=3, N.bins=20, xlab='predicted', ylab='Observed', main='gam')

calibration.plot(valData, which.model=4, N.bins=20, xlab='predicted', ylab='Observed', main='boot')

calibration.plot(valData, which.model=5, N.bins=20, xlab='predicted', ylab='Observed', main='rf')

calibration.plot(valData, which.model=6, N.bins=20, xlab='predicted', ylab='Observed', main='maxent')
```

Answer 1: 
Based off interpreting the graphs I would argue the Gam model does the best job because its points follow the line the best other than one hiccup at around 0.3; what I mean by this if we had a predicted value of 0.4 we observed a value of about 0.4. The next best competing models are likely the GLM or bootstrap models



# Challenge 2 (4 points)

Each SDM we created uses a different algorithm with different assumptions. Because of this, ecologists frequently use "ensemble" approaches that aggregate predictions from multiple models in some way. Here we are going to create an ensemble model by calculating a weighted average of the predicted occupancy values at each pixel. We will calculate weights based on model AUC values to ensure that the models with the best AUC values have the most influence on the predicted values in the ensemble model.

Create a raster stack that combines the glmMap, gamMap, boostMap, and rfMap (hint use c()).

Next, create a vector of the AUC values for each model.

Lastly, use the weighted.mean() function in the terra package to create the new raster as a weighted average of the previous 4 rasters.

Plot the result, and explain why we left out the bioclim and Maxent models for this ensemble model.


lets look at the plots again

```{r}
plot(glmMap)
plot(gamMap)
plot(boostMap)
plot(rfMap)
```

now lets stack them

```{r}
all=c(glmMap,gamMap,boostMap,rfMap)
```

calculate the auc values 

```{r}
summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2


for(i in 1:nModels){
  auc = auc(valData, which.model = i)
  summaryI = c(i, auc$AUC)
  summaryEval = rbind(summaryEval, summaryI)
}
summaryEval = summaryEval %>% 
  setNames(c('model', 'auc')) %>% 
  mutate(model = colnames(valData)[3:8])

summaryEval
```

now lets create a vector weight for the weighted mean function

```{r}
weights=c(summaryEval$auc[2:5])
```

now lets do a weighted mean raster


```{r}
wmall=terra::weighted.mean(all,weights)
plot(wmall)
```

Answer 2:
We left out the bioclim and maxent model because they both utilize presence only data with no absence or background points accounted for unlike the other 4 models utilized; we would be comparing apples to oranges.

# Challenge 3 (4 points)

Is this ensemble model an improvement over one of the models you built previously? Provide evidence and explain the criteria you used to come to your conclusion.

Extract the values

```{r}
ensembleval = terra::extract(wmall, vathValXy)

```

Make a new validation dataframe

```{r}

tmp2 = valCovs %>% mutate(VATH = vathVal$VATH)

valDataens = data.frame('ID' = 1:nrow(tmp2)) %>% 
  mutate(obs = tmp2$VATH,ensembleval)
```

Run a auc function

```{r}
summaryEvalens = data.frame(matrix(nrow=0, ncol=3))

nModelsens = ncol(valDataens)-2


for(i in 1:nModelsens){
  aucens = auc(valDataens, which.model = 1)
  summaryIens = c(2, auc$AUC)
  summaryEvalens = rbind(summaryEvalens, summaryIens)
}
summaryEvalens = summaryEvalens %>% 
  setNames(c('model', 'auc')) %>% 
  mutate(model = colnames(valDataens)[3])

summaryEvalens
```



Answer 3:

Based off the Auc of this model it is comparable with the AUC's of the prior models, it actaully isn't the best model because the prior GLM had a higher value (auc=0.67).


# Challenge 4 (4 points)

In the lab we built models using presence-background data then validated those models with presence-absence data. For this challenge, you're going to compare the predictive ability of a model built using presence-background data with one built using presence-absence data.

Fit a GLM using the presence-background data as we did in the lab (i.e., use the presBackCovs dataframe). Fit a second GLM using the presence-absence data (i.e., use the presAbsCovs dataframe). Validate both of these models on the novel presence-absence data (valCovs dataset). Specifically, calculate and compare AUC, Kappa, and TSS for these two models. Which model does a better job of prediction for the validation data and why do you think that is? 

Run the original glm

```{r}
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')
```


Now run the new glm

```{r}
glmModel2 = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presAbsCovs)

glmMap2 = predict(layers, glmModel2, type='response')
```

create a validation datadframe

```{r}

valData2 = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         glmVal2 = predict(glmModel2, tmp %>% select(canopy:precip), type='response'))

```


Run validation statistics 

```{r}
summaryEval2 = data.frame(matrix(nrow=0, ncol=9))

nModels2 = ncol(valData2)-2


for(i in 1:nModels2){
  
  #AUC
  auc = auc(valData2, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData2, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData2, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData2, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData2, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData2[,2], valData2[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData2[,i+2]*valData2[,2] + (1-valData2[,i+2]) * (1-valData2[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData2[,i+2] + 0.01)*valData2[,2] + log((1-valData2[,i+2]))*(1-valData2[,2])), ll)
  
  #Put them all together and save the values
  summaryI2 = c(i, auc$AUC, kappaOpt[[2]], tss, kappa[[1]])
  summaryEval2 = rbind(summaryEval2, summaryI2)
}

summaryEval2 = summaryEval2 %>% 
  setNames(c('model', 'auc', 'threshold', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData2)[3:4])

summaryEval2
```

Answer 4:
Starting with our AUC value, we can see the presence-absence data model (glmval2) has a better score where it has predictive ability slightly above random. Based on tss the presence-absence data model (glmval2) has a better tss score. While it is not great it indicates our performance was above just the odds of random chance. Regarding kappa both models are not improved from random chance but the background point model (glmval) has a slightly higher value. To conclude both models do a pretty bad job at predicting varied thrush presence however I am going to pick the presence-absence model (glmval2) because 1) it actually utilizes abscence data and 2) it has a higher auc and tss score. You may note that glmval model has a better kappa but as mentioned in class kappa may be biased based off sample size of validation data which is a fairly large data set. 


# Challenge 5 (4 points)

Now calculate the same statistics (AUC, Kappa, and TSS) for each model you developed in Challenge 4 using K-fold validation with 5 groups. Do these models perform better or worse based on K-fold validation (when compared with the validation metrics you calculated on a novel dataset in Challenge 2)? Why might that occur?

create some nfolds

```{r}
set.seed(23)

nFolds = 5
kfoldPres = kfold(presCovs, k=nFolds)
kfoldBack = kfold(backCovs, k=nFolds)
```

run a k-fold validation of the first model

```{r}
for(i in 1:nFolds){
  valPres = presCovs[kfoldPres==i,]
  valBack = backCovs[kfoldBack==i,]
  valboth = rbind(valPres,valBack)
  
  trainPres1 = presCovs[kfoldPres!=i,]
  trainBack1 = backCovs[kfoldBack!=i,]
  trainBoth1 = rbind(trainPres1, trainBack1)
  
  
  glmModel1k = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth1)

  valData3 = data.frame('ID' = 1:nrow(valboth))%>% 
  mutate(obs = valboth$pres, glmVal1 = predict(glmModel1k, valboth %>% select(canopy:precip), type='response'))
}


```

now setup a kfold for the second model

```{r}
set.seed(23)

nFolds = 5
kfoldPres = kfold(presCovs, k=nFolds)
kfoldAbs = kfold(absCovs, k=nFolds)
```

now run the kfold for the second model

```{r}
for(i in 1:nFolds){
  valPres = presCovs[kfoldPres==i,]
  valAbs = absCovs[kfoldAbs==i,]
  valboth2 = rbind(valPres,valAbs)
  
  trainPres1 = presCovs[kfoldPres!=i,]
  trainAbs1 = absCovs[kfoldAbs!=i,]
  trainBoth2 = rbind(trainPres1, trainAbs1)
  
  
  glmModel2k = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth2)

  valData4 = data.frame('ID' = 1:nrow(valboth2))%>% 
  mutate(obs = valboth2$pres, glmVal2 = predict(glmModel2k, valboth2 %>% select(canopy:precip), type='response'))
}


```

calculate validation stats for first model

```{r}
summaryEval3 = data.frame(matrix(nrow=0, ncol=9))

nModels2 = ncol(valData3)-2


for(i in 1:nModels2){
  
  #AUC
  auc = auc(valData3, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData3, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData3, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData3, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData3, which.model = i, threshold = kappaOpt[[2]]))
  
  
  #Put them all together and save the values
  summaryI3 = c(i, auc$AUC, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval3 = rbind(summaryEval3, summaryI3)
}

summaryEval3 = summaryEval3 %>% 
  setNames(c('model', 'auc', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData3)[3])

summaryEval3
```

now calculate the validation stats of the second model

```{r}
summaryEval4 = data.frame(matrix(nrow=0, ncol=9))

nModels3 = ncol(valData4)-2


for(i in 1:nModels3){
  
  #AUC
  auc = auc(valData4, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData4, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData4, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData4, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData4, which.model = i, threshold = kappaOpt[[2]]))
  
  
  #Put them all together and save the values
  summaryI4 = c(i, auc$AUC, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval4 = rbind(summaryEval4, summaryI4)
}

summaryEval4 = summaryEval4 %>% 
  setNames(c('model', 'auc', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData4)[3])

summaryEval4
```

Answer 5:
Based of the validation stats the models both improved using the k-fold method for all validation stats (auc, tss, kappa), is it higher becuase k-fold method is primarily used for presence only data and with us running it with prescence and abscence data it results in bias?
