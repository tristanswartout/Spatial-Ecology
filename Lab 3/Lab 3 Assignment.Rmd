---
title: "Tristan Swartout Lab 3 Assignment - Scale"
output: html_notebook
---

First lets run all needed packages 
```{r}
require(sf)
require(AICcmodavg)
require(tigris)
require(FedData)
require(tidyverse)
require(terra)
```

## Challenge 1 (4 points)

**Build a raster with 100 rows and 100 columns. Fill the raster cells with values of a random variable drawn from a distribution of your choosing (Poisson, Normal, Uniform, etc.). Calculate the mean and variance of the values in that raster. Now increase the grain size of those cells by factors of 2, 5, and 10, combining cell values using a mean function. At each iteration, calculate the mean and variance of the values in the resulting raster. Generate 2 scatterplots that have grain size on the x-axis. Plot the mean raster value on the y-axis of the first, and variance on the y-axis of the second. What do you notice about how these values change as you "scale up" the grain size? Why do you think this pattern occurs?**

Create a raster

```{r}
simpRast = rast(ncol=100, nrow=100, xmin=1, xmax=100, ymin=1, ymax=100)
plot(simpRast)

```
Just like the lab example, we do not get an output
Lets actually set up the raster 

```{r}
set.seed(23)
simpRast[] = rpois(ncell(simpRast), lambda=3)

plot(simpRast)
text(simpRast, digits=2)
```
Here is the raw raster
Now lets increase grain by a factor of 2 and have cells fuse by the mean.

```{r}
simpRast2<- aggregate(simpRast, fact=2, fun='mean')

plot(simpRast2)
text(simpRast2, digits=2)
```
Now increase by a factor of 5. 
```{r}
simpRast5<- aggregate(simpRast, fact=5, fun='mean')

plot(simpRast5)
text(simpRast5, digits=2)
```

Now increase by a factor of 10. 
```{r}
simpRast10<- aggregate(simpRast, fact=10, fun='mean')

plot(simpRast10)
text(simpRast10, digits=2)

```
Now let's see the variance and mean of all these rasters.

```{r}
simpRastmean=mean(as.matrix(simpRast))
simpRastvar=var(as.matrix(simpRast))

simpRast2mean=mean(as.matrix(simpRast2))
simpRast2var=var(as.matrix(simpRast2))

simpRast5mean=mean(as.matrix(simpRast5))
simpRast5var=var(as.matrix(simpRast5))

simpRast10mean=mean(as.matrix(simpRast10))
simpRast10var=var(as.matrix(simpRast10))

simpRastmean
simpRastvar
simpRast2mean
simpRast2var
simpRast5mean
simpRast5var
simpRast10mean
simpRast10var
```

Now based off these variables lets create a vector for each so we can graph the trends.

```{r}
Grainmeans=c(3.0078,3.0078,3.0078)
Grainvalues=c(2,5,10)
Grainvariance=c(0.7234786,0.1325425,0.02759511)
```
Now lets plot it
```{r}
plot(Grainmeans~Grainvalues)
plot(Grainvariance~Grainvalues)
```

Answer 1:
What you will first notice is the mean does not change even with the grain scaling up for each raster while the variance decreases with an increase in grain. The trend observed for the mean does not change because yes while the number of raster grain values change they are merged together and form a new value which is the mean of all the former values fused together; resulting in no change in overall mean. Regarding the decreasing trend in variance; this is occurring because the variability between mean values is decreasing. As there are fewer values to calculate the mean it should hypothetically approach closer to the actual mean of the raster which in this case is 3.0078. 

## Challenge 2 (4 points)

**Identify a situation in which you might use a summary function other than the mean to calculate new cell values when you scale up the grain of a raster (e.g., median, mode, minimum, maximum, etc.). Repeat the effort from Challenge 1 using this alternate function. Again, create two scatterplots showing how the mean and variance values of the raster change as you scale up the cell size by factors of 2, 5, and 10. Do you see a similar pattern? Compare and contrast your findings with those from Challenge 1.**

*Hint: You should be able to recycle your code from Challenge 1 with only a couple of small tweaks to answer this question.*

Let's make a raster again.

```{r}
Rast = rast(ncol=100, nrow=100, xmin=1, xmax=100, ymin=1, ymax=100)
plot(Rast)

```

Now to finalize setup of the raster.
```{r}
set.seed(23)
Rast[] = rpois(ncell(Rast), lambda=3)

plot(Rast)
text(Rast, digits=2)
```

Let's increase grain by a factor of 2 with cell fusion based off mode.
```{r}
Rast2<- aggregate(Rast, fact=2, fun='modal')

plot(Rast2)
text(Rast2, digits=2)
```
Now increase by a factor of 5.
```{r}
Rast5<- aggregate(Rast, fact=5, fun='modal')

plot(Rast5)
text(Rast5, digits=2)
```
Now increase by a factor of 10
```{r}
Rast10<- aggregate(Rast, fact=10, fun='modal')

plot(Rast10)
text(Rast10, digits=2)

```
Now calculate mean and variance for each raster.
```{r}
Rastmean=mean(as.matrix(Rast))
Rastvar=var(as.matrix(Rast))

Rast2mean=mean(as.matrix(Rast2))
Rast2var=var(as.matrix(Rast2))

Rast5mean=mean(as.matrix(Rast5))
Rast5var=var(as.matrix(Rast5))

Rast10mean=mean(as.matrix(Rast10))
Rast10var=var(as.matrix(Rast10))

Rastmean
Rastvar
Rast2mean
Rast2var
Rast5mean
Rast5var
Rast10mean
Rast10var
```
Now let's create vectors to plot
```{r}
Grainmeans2=c(2.1988,2.54,2.67)
Grainvalues2=c(2,5,10)
Grainvariance2=c(2.063304,0.8655639,0.4657576)
```

Let's plot the vectors.
```{r}
plot(Grainmeans2~Grainvalues2)
plot(Grainvariance2~Grainvalues2)
```

Answer 2:
Other than mean a common summary output is mode and this could be useful in situations such as land cover type where you are trying to determine the most common cover type for an area based off raster cell frequency. There are some differences between the findings of challenge 1 and 2. A similarity between Challenge 1 and 2 is that in both cases the variance decreases with an increase in grain size with each raster. Just like the first challenge this is occurring because there is less variability as the the grain size increases. A key difference between challenge 1 and 2 is that the mean increases in challenge 2 while it remains constant in challenge 1. This is likely because with modal function you are not calculating a new mean from 4 cells but the most common integer which will result in a different mean with changing grain size. 

## Challenge 3 (2 points)

**Recall that before we calculated forest cover, we cropped our NLCD raster to minimize its size and the computing effort necessary from our poor little computers. How might that affect our ability to evaluate the scale at which five-lined skinks respond to forest cover? Why?**

Answer 3:
By cropping the area we are scaling the data and assuming we picked the correct scale. As documented in Thursday's discussion course, determining the right scale is a difficult task. In the large raster file we took our study area and created an extent of it with 10 km on each side, then we did 5 km and 10 km buffers around each point. While these may be the correct choices they could also be incorrect and either having far to large of scales or possibly to small of scales causing issues with doing analysis interpretation.

## Challenge 4 (4 points)

**In the lab, we measured forest cover at 1 km and 5 km. Extract forest cover proportions around each sample point for 100 m, 500 m, 1 km, 2 km, 3 km, 4 km, and 5 km scales. Examine the correlation between these 7 variables (remember the chart.Correlation() function). What patterns do you notice in correlation among these variables?**

*Hint: Recall the for loop we used to calculate this variable at two scales... could you make a small addition here to look at more scales?*

Let's download the data and create a map of the SE states.

```{r, warning=F, message=F}


sites = st_read("/vsicurl/https://github.com/ValenteJJ/SpatialEcology/raw/main/Week3/reptiledata.shp") %>% 
  filter(management!='Corn')
st_crs(sites) = "+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"
head(sites)


states = states() %>% 
  filter(NAME %in% c('Alabama', 'Florida', 'Georgia')) %>% 
  st_transform(crs(sites, proj=T))



ggplot()+
  geom_sf(data = states)+
  geom_sf(data = sites)
```

Now lets add additional data.

```{r}
presAbs = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week3/reptiles_flsk.csv')

sites = sites %>% 
  left_join(presAbs, by='site')
```

Lets create an extent of the area just like the lab example.
```{r}

#Extract x and y coordinates of the bounding box
studyArea = st_bbox(sites) + c(-10000, -10000, 10000, 10000)
studyArea = st_as_sfc(studyArea)


ggplot()+
  geom_sf(data = states)+
  geom_sf(data = studyArea, fill=NA, color='red')+
  geom_sf(data = sites)

```
Now lets download the NLCD covertype raster
```{r}
nlcd = get_nlcd(studyArea,
                label='studyArea',
                year = 2016,
                dataset = 'landcover',
                landmass = 'L48'
)

plot(nlcd, 1, legend=T, plg=list(cex=0.5))
plot(st_geometry(sites), add=T, pch=16)
```
Now lets extract just forest from the covertype file.
```{r}

forest = nlcd %>% 
  setValues(0)

forest[nlcd=='Deciduous Forest' | nlcd=='Evergreen Forest' | nlcd=='Mixed Forest'] = 1
plot(forest)
plot(st_geometry(sites), add=T, pch=16, col='black')
```
Now we need to create buffers around each point at multi distance scales

```{r}
buffSite100m = st_buffer(sites[], dist=100)
buffSite500m = st_buffer(sites[], dist=500)
buffSite1km = st_buffer(sites[], dist=1000)
buffSite2km = st_buffer(sites[], dist=2000)
buffSite3km = st_buffer(sites[], dist=3000)
buffSite4km = st_buffer(sites[], dist=4000)
buffSite5km = st_buffer(sites[], dist=5000)

```

Now with the loop function Jonathon created, lets calculate forest of the area based off cell proportion.

```{r}


bufferCover = function(shp, size, landcover){
  buffArea = (pi*size^2)/10000
  grainArea = (prod(res(landcover)))/10000
  
  buffi = st_buffer(shp[i,], dist=size)
  cropi = crop(landcover, buffi, mask=T)
  numCells = global(cropi, 'sum', na.rm=T)
  forestHa = numCells * grainArea
  propForest = forestHa / buffArea
  
  return(propForest)
}


#This is where we are going to store the output values
for100m = as.vector(rep(NA, nrow(sites)))
for500m = as.vector(rep(NA, nrow(sites)))
for1km = as.vector(rep(NA, nrow(sites)))
for2km = as.vector(rep(NA, nrow(sites)))
for3km = as.vector(rep(NA, nrow(sites)))
for4km = as.vector(rep(NA, nrow(sites)))
for5km = as.vector(rep(NA, nrow(sites)))

for(i in 1:nrow(sites)){
  for100m[i] = bufferCover(sites, 100, forest)
  for500m[i] = bufferCover(sites, 500, forest)
  for1km[i] = bufferCover(sites, 1000, forest)
  for2km[i] = bufferCover(sites, 2000, forest)
  for3km[i] = bufferCover(sites, 3000, forest)
  for4km[i] = bufferCover(sites, 4000, forest)
  for5km[i] = bufferCover(sites, 5000, forest)
}

forestData = sites %>% 
  mutate(for100m = unlist(for100m),for500m = unlist(for500m),for1km = unlist(for1km),for2km = unlist(for2km),for3km = unlist(for3km),for4km = unlist(for4km),for5km = unlist(for5km))

head(forestData)
```
Now with all things set let's run a chart correlation comparing all of our buffer variables. 

```{r}

forestData %>% 
  as.data.frame() %>% 
  select(coords_x1, for100m, for500m, for1km, for2km, for3km, for4km, for5km) %>% 
  PerformanceAnalytics::chart.Correlation(histogram=F)

```
Answer 4:

Based off the correlation chart you may notice pretty strong positive correlation between these variables. For example, when radius variables are close in distance (e.g., 4 km and 5 km) they are extremely correlated up to a correlation value of 0.98. While in the opposite case when the radius variables are far in distance such a 100 m and 5km there is very little correlation; a value of 0.10. This is not true in all cases, for example, 2 km and 5 km may be fairly different in scale but still highly correlated. To summarize, if two variables are close in scale they will generally have high correlation (0.75 or higher) while variables far in scale from each other will generally have little to no correlation. 

## Challenge 5 (4 points)

**Fit 8 logistic regression models (a null model and one for each of the 7 forest scales). Compare these models using AICc. Which scale do you think represents the critical or characteristic scale at which forest cover affects skink presence? Is this scale clearly better than the others, or is there some ambiguity? What are some mechanisms by which forest cover could affect skink presence at this scale? What is your overall conclusion regarding how forest cover affects skink presence (i.e., take a look at the betas)?**

Lets create a model for each buffer scale (including a null model). 
Then lets run a AIC table to see what our competing models are.

```{r}

modelNull = glm(pres~1, family='binomial', data=forestData)
model100m = glm(pres~for100m, family='binomial', data=forestData)
model500m = glm(pres~for500m, family='binomial', data=forestData)
model1km = glm(pres~for1km, family='binomial', data=forestData)
model2km = glm(pres~for2km, family='binomial', data=forestData)
model3km = glm(pres~for3km, family='binomial', data=forestData)
model4km = glm(pres~for4km, family='binomial', data=forestData)
model5km = glm(pres~for5km, family='binomial', data=forestData)

aictab(list(modelNull,model100m,model500m,model1km,model2km,model3km,model4km,model5km), modnames=c('Null', '100 m', '500 m', '1 km', '2 km', '3 km', '4 km', '5 km'))

```
Lets look at the betas for these models

```{r}
model100m
model500m
model1km
model2km
model3km
model4km
model5km

exp(10.492)
exp(10.051)
exp(10.716)
exp(10.149)

```

Answer 5:

Based off the AIC table, the 2-km model is the most supported based off the lowest delta AIC score and AIC weight. However, there are clearly competing models, both 4, 3, and 5 km models are within 2 delta AIC scores which would indicate these models may be just as likely as being the appropriate model regarding what scale to use. Finally, it is important to notice the top model only has a cumulative weight of 0.36 which is fairly low this indicates a better model may include more variables or other metrics not accounted for. Interesting to note, these scale are all fairly large for such a small species; 5-km is quite a buffer of forests for a tiny species but 2-km may also be a top competitor that's a fairly large difference in distance. When looking at the betas for these top models youll notice that after exponentiating the beta there is a large difference in odds ratio. For example, the beta for 2-km is 36026 odds ratio (times as likely) while the odds ratio for 4-km is a odds ratio of 45071 (times as likely) these are huge differences and must be considered when what model may be best for management decisions. Regardless forest cover provides benefit to skink presence; if there is forest then skinks are more likely to be present. 

## Challenge 6 (2 points)

**If you encounter ambiguity in identifying the characteristic scale of an effect, can you come up with a clever way to condense the information in the multi-scale variables into just one or two? When might it be ok to include two covariates in the same model (think multiple regression) that represent the same ecological feature measured at different scales (e.g., forest cover at 1 km AND forest cover at 5 km in the same model)? I can think of both a biological and a statistical answer to this question.**

Answer 6:

As I mentioned at the end of the last challenge question, the top model has competitors and a low weight. A possible solution to improve fit is to have a multi-regression model where you include several scale variables together (i.e., 2, 4, and 3 km all in same model). Now in real life would this make sense to do in real life? If you are curious about an animals behavior at several scales; then yes it make make sense. I can definitely think of a scenario in the case of skinks where maybe you are looking at skink movement in their core area and also their max home range where you want to account for both regions of skink movement; so as a result, you would run a model where both variables are included especially if an AIC table indicates its a top model. 
